{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVwdamuZ+Pzt6F481bRVqM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya1110/ai_robotics_lab_2025_hands_on/blob/main/Week03_Simple_SGD_Example__with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3N0fVQqIH-w"
      },
      "source": [
        "# Simple SGD Example with PyTorch\n",
        "\n",
        "This notebook explains how the weights and the biases of neural network models are optimized using the stochastic gradient descent (SGD) method.\n",
        "\n",
        "In this example, we will implement a simple linear regression model with PyTorch.\n",
        "\n",
        "First, let's import the PyTorch, NumPy, and Matplotlib packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVx3kr0D4Ppr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFGAxPumJKhm"
      },
      "source": [
        "Here, we create a dataset consisting of $x$ (inputs) and $y$ (outputs) using a simple linear equation below. It's important to note that the output data $y$ contains some random noise.\n",
        "\n",
        "$y = 5x + 3 + \\mathrm{noise}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d12jiZah4lAV"
      },
      "source": [
        "x = 10*np.random.rand(100)-5\n",
        "noise = 3*np.random.randn(x.shape[0])\n",
        "y = 5*x + 3 + noise\n",
        "\n",
        "plt.plot(x, y, marker=\"o\", lw= 0, label=\"data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1d9TcZJKOAH"
      },
      "source": [
        "## Least Squares Polynominal Fit\n",
        "\n",
        "Our objective is to discover a linear function model (equation) that can effectively capture the relationship between the $x$ and $y$ dataset.\n",
        "\n",
        "In fact, instead of employing PyTorch, we can use `np.polyfit()` to fit the dataset. This allows us to acquire the fitting parameters $w$ and $b$ for the linear function of $y = wx + b$, where $w$ and $b$ are referred to as the weight and bias, respectively.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REAk1xWzG4cs"
      },
      "source": [
        "w, b = np.polyfit(x, y, 1)\n",
        "print(f\"w={w:.3f}, b={b:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beBJ2jmvOnsi"
      },
      "source": [
        "Due to the presence of noise in the $x-y$ dataset, the obtained values of $w$ and $b$ may not match the exact values used to create the dataset. Nevertheless, they are typically close enough to allow us to create a fitting line using these parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIJCGZ_5KD8"
      },
      "source": [
        "y_fit = w*x + b\n",
        "\n",
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"dataset\")\n",
        "plt.plot(x, y_fit, lw=2, label=\"fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAT9dny8FhRW"
      },
      "source": [
        "## Stocastic Gradient Descent\n",
        "\n",
        "While `np.polyfit()` works effectively, in this example, we will achieve the same outcome using PyTorch.\n",
        "\n",
        "Let's start by converting the $x$ and $y$ dataset into `torch.tensor` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mbf0APf5yYF"
      },
      "source": [
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "print(type(x))\n",
        "print(type(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWY-KZbAJopr"
      },
      "source": [
        "Next, we define a function named `model()` that outputs the predicted value `p` from an input value `x` using the parameters `w` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-FhluBo7Ea1"
      },
      "source": [
        "def model(x):\n",
        "    p = w*x + b\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vemQxk48K_mz"
      },
      "source": [
        "We also define a function named `loss_func()` to calculate the mean squared error between `p` and `y`. This type of function is referred to as a loss function, which helps measure the degree of error in the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3Ll8sEkMb6"
      },
      "source": [
        "$\\displaystyle \\mathrm{loss} = \\mathrm{mse}(p, y) = \\frac{1}{N}\\sum_{i=0}^{N-1}(p_i-y_i)^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJX-xDCl7Le1"
      },
      "source": [
        "def loss_func(p, y):\n",
        "    loss = ((p-y)**2).mean()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZdLCxF8NkXD"
      },
      "source": [
        "At this stage, we haven't determined the values of `w` and `b` yet. Therefore, we initialize these values with arbitrary numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_fsb7vWO6Dy"
      },
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)   # you can set any number here\n",
        "b = torch.tensor(-5.0, requires_grad=True)  # you can set any number here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_4S2EQYO8wl"
      },
      "source": [
        "Now, we can make a prediction using `model()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLHoWRI7O8Rz"
      },
      "source": [
        "p = model(x)\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtXwQ3nIttbt"
      },
      "source": [
        "Let's visualize the current prediction. Since we've initialized `w` and `b` with arbitrary numbers, it's expected that the model's prediction won't fit the data well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvw_X_MPKhL"
      },
      "source": [
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"data\")\n",
        "plt.plot(x, p.detach().numpy(), label=\"prediction\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7CxsLjPsnR"
      },
      "source": [
        "Next, we calculate the loss value (mean squared error) using `loss_func()`. It's important to note that the loss value will be very large because `w` and `b` are arbitrary values and have not been optimized yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIXfD63wPzVm"
      },
      "source": [
        "loss = loss_func(p, y)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roLiOg-dQp_q"
      },
      "source": [
        "To optimize `w` and `b`, we must determine the gradients of the loss with respect to the current values of `w` and `b`. This can be achieved using `loss.backward()`. The gradients, denoted as $\\displaystyle \\frac{\\partial \\mathrm{loss}}{\\partial w}$ and $\\displaystyle \\frac{\\partial \\mathrm{loss}}{\\partial b}$, can be accessed by `w.grad` and `b.grad`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNT7UuCnQh2X"
      },
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPnNfKsTLW_"
      },
      "source": [
        "We can update `w` and `b` using the following equations, where $\\eta$ represents the learning rate. This method is known as stochastic gradient descent (SGD):\n",
        "\n",
        "$\\displaystyle w := w - \\eta\\frac{\\partial\\mathrm{loss}}{\\partial w}$\n",
        "\n",
        "$\\displaystyle b := b - \\eta\\frac{\\partial\\mathrm{loss}}{\\partial b}$\n",
        "\n",
        "When updating these values, we don't want to compute gradients. To achieve this, we use with `torch.no_grad()` at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpoxdQ0WTK9X"
      },
      "source": [
        "lr = 0.01    # define learning rate\n",
        "\n",
        "with torch.no_grad():    # disable gradients calculations\n",
        "    w -= w.grad*lr       # update w\n",
        "    b -= b.grad*lr       # update b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aen0JWycU6BN"
      },
      "source": [
        "At this stage, you'll notice that the values of `w` and `b` are closer to their true values ($w$=5.0, $b$=3.0) compared to the initial values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6yLMDgU4Yr"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G7G6xIpYnlk"
      },
      "source": [
        "To further optimize `w` and `b`, we repeat the above process multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVJCrKHR7UPk"
      },
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)   # you can set any number here\n",
        "b = torch.tensor(-5.0, requires_grad=True)  # you can set any number here\n",
        "\n",
        "lr = 0.01    # learning rate\n",
        "epochs = 300  # how many times we repeat training\n",
        "\n",
        "w = torch.tensor(3.0, requires_grad=True)    # initialize w\n",
        "b = torch.tensor(-1.0, requires_grad=True)   # initialize b\n",
        "\n",
        "# empty lists for saving loss, w, b\n",
        "loss_list = []\n",
        "w_list = []\n",
        "b_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    p = model(x)              # prediction\n",
        "    loss = loss_func(p, y)    # measure loss\n",
        "    loss.backward()           # determine gradients\n",
        "\n",
        "    with torch.no_grad():     # disable autograd\n",
        "        w -= w.grad*lr        # update w\n",
        "        b -= b.grad*lr        # update b\n",
        "\n",
        "        w.grad.zero_() # reset gradient\n",
        "        b.grad.zero_() # reset gradient\n",
        "\n",
        "    # save loss, w, b\n",
        "    loss_list.append(loss.item())\n",
        "    w_list.append(w.item())\n",
        "    b_list.append(b.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, loss={loss.item():.3f}, w={w.item():.3f}, b={b.item():.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wmY5X4VF8I"
      },
      "source": [
        "As you can see now, `w` is now close to 5.0, and `b` is close to 3.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLVOe3NUHXZ"
      },
      "source": [
        "Let's visualize how the loss value has decreased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKNMYl8W-YPu"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, loss_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuWi5tIEUNty"
      },
      "source": [
        "Let's visualize how the `w` value has changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9-IoVMT5dB"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, w_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"w\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NimP-NFUW7p"
      },
      "source": [
        "Let's visualize how the `b` value has changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hgnOPXWUA_p"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, b_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"b\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the optimized values of `w` and `b`, the model fits the dataset very well."
      ],
      "metadata": {
        "id": "ZbWEnmyEEwEE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYXitcRHQQ9L"
      },
      "source": [
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"dataset\")\n",
        "plt.plot(x, p.detach().numpy(), label=\"prediction\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU83IdbgUqfo"
      },
      "source": [
        "Now you can try to change initial values of `epochs`, `lr`, `w`, `b` etc., and let's observe the results."
      ]
    }
  ]
}